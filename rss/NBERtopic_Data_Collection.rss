<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>RSS for NBER topic Data Collection</title><link>a link</link><description>RSS for NBER topic Data Collection</description><language>en-US</language><lastBuildDate>Tue, 14 Dec 2021 13:25:36 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Selection in Surveys</title><link>https://www.nber.org/papers/w29549</link><description><![CDATA[<b>December 2021</b> <p> We evaluate how nonresponse affects conclusions drawn from survey data and consider how researchers can reliably test and correct for nonresponse bias. To do so, we examine a survey on labor market conditions during the COVID-19 pandemic that used randomly assigned financial incentives to encourage participation. We link the survey data to administrative data sources, allowing us to observe a ground truth for participants and nonparticipants. We find evidence of large nonresponse bias, even after correcting for observable differences between participants and nonparticipants. We apply a range of existing methods that account for nonresponse bias due to unobserved differences, including worst-case bounds, bounds that incorporate monotonicity assumptions, and approaches based on parametric and nonparametric selection models. These methods produce bounds (or point estimates) that are either too wide to be useful or far from the ground truth. We show how these shortcomings can be addressed by modeling how nonparticipation can be both active (declining to participate) and passive (not seeing the survey invitation). The model makes use of variation from the randomly assigned financial incentives, as well as the timing of reminder emails. Applying the model to our data produces bounds (or point estimates) that are narrower and closer to the ground truth than the other methods.]]></description><author>Deniz Dutz, Ingrid Huitfeldt, Santiago Lacouture, Magne Mogstad, Alexander Torgovitsky, Winnie van Dijk</author><guid isPermaLink="true">https://www.nber.org/papers/w29549</guid></item><item><title>Modeling to Inform Economy-Wide Pandemic Policy: Bringing Epidemiologists and Economists Together</title><link>https://www.nber.org/papers/w29475</link><description><![CDATA[<b>November 2021</b> <p> Facing unprecedented uncertainty and drastic trade-offs between public health and other forms of human well-being, policy makers during the Covid-19 pandemic have sought the guidance of epidemiologists and economists. Unfortunately, while both groups of scientists use many of the same basic mathematical tools, the models they develop to inform policy tend to rely on different sets of assumptions and, thus, often lead to different policy conclusions. This divergence in policy recommendations can lead to uncertainty and confusion, opening the door to disinformation, distrust of institutions, and politicization of scientific facts. Unfortunately, to date, there have not been widespread efforts to build bridges and find consensus or even to clarify sources of differences across these fields, members of whom often continue to work within their traditional academic silos. In response to this “crisis of communication,” we convened a group of scholars from epidemiology, economics, and related fields (e.g., statistics, engineering, and health policy) to discuss approaches to modeling economy-wide pandemics. We summarize these conversations by providing a consensus view of disciplinary differences (including critiques) and working through a specific policy example. Thereafter, we chart a path forward for more effective synergy between disciplines, which we hope will lead to better policies as the current pandemic evolves and future pandemics emerge.]]></description><author>Michael E. Darden, David Dowdy, Lauren Gardner, Barton Hamilton, Karen Kopecky, Melissa Marx, Nicholas W. Papageorge, Daniel Polsky, Kimberly Powers, Elizabeth Stuart, Matthew Zahn</author><guid isPermaLink="true">https://www.nber.org/papers/w29475</guid></item><item><title>Uncertainty and Change: Survey Evidence of Firms' Subjective Beliefs</title><link>https://www.nber.org/papers/w29430</link><description><![CDATA[<b>November 2021</b> <p> This paper studies how managers plan under uncertainty. In a new survey panel on German manufacturing firms, we show that uncertainty reflects change: Planning incorporates higher subjective uncertainty about future sales growth when the firm has just experienced unusual growth, and more so if the experience was negative. At the quarterly frequency, subjective uncertainty closely tracks conditional volatility of shocks: Both exhibit an asymmetric V-shaped relationship with past growth. In the cross section of firms, however, subjective uncertainty differs from conditional volatility: planning in successful firms—either large or fast-growing—reflects lower subjective uncertainty than in unsuccessful firms even when the size of the shocks is the same.]]></description><author>Ruediger Bachmann, Kai Carstensen, Stefan Lautenbacher, Martin Schneider</author><guid isPermaLink="true">https://www.nber.org/papers/w29430</guid></item><item><title>The Anti-Poverty, Targeting, and Labor Supply Effects of the Proposed Child Tax Credit Expansion</title><link>https://www.nber.org/papers/w29366</link><description><![CDATA[<b>October 2021</b> <p> The proposed change under the American Families Plan (AFP) to the Tax Cuts and Jobs Act (TCJA) Child Tax Credit (CTC) would increase maximum benefit amounts to $3,000 or $3,600 per child (up from $2,000 per child) and make the full credit available to all low and middle-income families regardless of earnings or income. We estimate the anti-poverty, targeting, and labor supply effects of the expansion by linking survey data with administrative tax and government program data which form part of the Comprehensive Income Dataset (CID). Initially ignoring any behavioral responses, we estimate that the expansion of the CTC would reduce child poverty by 34% and deep child poverty by 39%. The expansion of the CTC would have a larger anti-poverty effect on children than any existing government program, though at a higher cost per child raised above the poverty line than any other means-tested program. Relatedly, the CTC expansion would allocate a smaller share of its total dollars to families at the bottom of the income distribution—as well as families with the lowest levels of long-term income, education, or health—than any existing means-tested program with the exception of housing assistance. We then simulate anti-poverty effects accounting for labor supply responses. By replacing the TCJA CTC (which contained substantial work incentives akin to the Earned Income Tax Credit) with a universal basic income-type benefit, the CTC expansion reduces the return to working at all by at least $2,000 per child for most workers with children. Relying on elasticity estimates consistent with mainstream simulation models and the academic literature, we estimate that this change in policy would lead 1.5 million workers (constituting 2.6% of all working parents) to exit the labor force. The decline in employment and the consequent earnings loss would mean that child poverty would only fall by 22% and deep child poverty would not fall at all with the CTC expansion.]]></description><author>Kevin Corinth, Bruce D. Meyer, Matthew Stadnicki, Derek Wu</author><guid isPermaLink="true">https://www.nber.org/papers/w29366</guid></item><item><title>Using Household Rosters from Survey Data to Estimate All-cause Mortality during COVID in India</title><link>https://www.nber.org/papers/w29192</link><description><![CDATA[<b>August 2021</b> <p> Official statistics on deaths in India during the COVID pandemic are either incomplete or are reported with a delay.  To overcome this shortcoming, we estimate excess deaths in India using the household roster from a large panel data set, the Consumer Pyramids Household Survey, which reports attrition from death.  We address the problem that the exact timing of death is not reported in two ways, via a moving average and differencing monthly deaths.  We estimate roughly 4.5 million (95% CI: 2.8M to 6.2M) excess deaths over 16 months during the pandemic in India.  While we cannot demonstrate causality between COVID and excess deaths, the pattern of excess deaths is consistent with COVID-associated mortality.  Excess deaths peaked roughly during the two COVID waves in India; the age structure of excess deaths is right skewed relative to baseline, consistent with COVID infection fatality rates; and excess deaths are positively correlated with reported infections.  Finally, we find that the incidence of excess deaths was disproportionately among the highest tercile of income-earners and was negatively associated with district-level mobility.]]></description><author>Anup Malani, Sabareesh Ramachandran</author><guid isPermaLink="true">https://www.nber.org/papers/w29192</guid></item><item><title>Errors in Reporting and Imputation of Government Benefits and Their Implications</title><link>https://www.nber.org/papers/w29184</link><description><![CDATA[<b>August 2021</b> <p> We document the extent, nature, and consequences of survey errors in cash welfare and SNAP receipt in three major U.S. household surveys. We find high rates of misreporting, particularly failure to report receipt. The surveys inaccurately capture patterns of multiple program participation, even though there is little evidence of program confusion. Error rates are higher among imputed observations, which account for a large share of false positive errors. Many household characteristics have significant effects on both false positives and false negative errors. Error rates sharply differ by race, ethnicity, income and other household characteristics.  The errors greatly affect models of program receipt and estimated effects of income and race are noticeably biased. We examine error due to item non-response and imputation, as well as whether imputation improves estimates. Item non-respondents have higher receipt rates than the population conditional on covariates. The assumptions for consistent estimates in multivariate models fail both when excluding item non-respondents and when using the imputed values.  In binary choice models of program receipt, linked data estimates favor excluding item non-respondents rather than using imputed values. Biases are well predicted by the error patterns we document, helping researchers make informed decisions on whether to use imputed values.]]></description><author>Pablo A. Celhay, Bruce D. Meyer, Nikolas Mittag</author><guid isPermaLink="true">https://www.nber.org/papers/w29184</guid></item></channel></rss>