<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>RSS by programs: Technical Working Papers</title><link>a link</link><description>RSS by programs: Technical Working Papers</description><language>en-US</language><lastBuildDate>Thu, 10 Feb 2022 10:55:43 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>When is TSLS Actually LATE?</title><link>https://www.nber.org/papers/w29709</link><description><![CDATA[<i>Christine Blandhol, John Bonney, Magne Mogstad, Alexander Torgovitsky.</i> <p> <b>January 2022.</b> <p> Linear instrumental variable estimators, such as two-stage least squares (TSLS), are commonly interpreted as estimating positively weighted averages of causal effects, referred to as local average treatment effects (LATEs). We examine whether the LATE interpretation actually applies to the types of TSLS specifications that are used in practice. We show that if the specification includes covariates, which most empirical work does, then the LATE interpretation does not apply in general. Instead, the TSLS estimator will in general reflect treatment effects for both compliers and always/never-takers, and some of the treatment effects for the always/never-takers will necessarily be negatively weighted. We show that the only specifications that have a LATE interpretation are "saturated" specifications that control for covariates nonparametrically, implying that such specifications are both sufficient and necessary for TSLS to have a LATE interpretation, at least without additional parametric assumptions. This result is concerning because, as we document, empirical researchers almost never control for covariates nonparametrically, and rarely discuss or justify parametric specifications of covariates. We develop a decomposition that quantifies the extent to which the usual LATE interpretation fails. We apply the decomposition to four empirical analyses and find strong evidence that the LATE interpretation of TSLS is far from accurate for the types of specifications actually used in practice.]]></description><author>Christine Blandhol, John Bonney, Magne Mogstad, Alexander Torgovitsky</author><pubDate>Sat, 01 Jan 2022 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29709</guid></item><item><title>Addressing Endogeneity Using a Two-stage Copula Generated Regressor Approach</title><link>https://www.nber.org/papers/w29708</link><description><![CDATA[<i>Fan Yang, Yi Qian, Hui Xie.</i> <p> <b>January 2022.</b> <p> A prominent challenge when drawing causal inference using observational data is the ubiquitous presence of endogenous regressors. The classical econometric method to handle regressor endogeneity requires instrumental variables that must satisfy the stringent condition of exclusion restriction, making it infeasible to use in many settings. We propose new instrument-free methods using copulas to address the endogeneity problem. The existing copula correction method focuses only on the endogenous regressors and may yield biased estimates when exogenous and endogenous regressors are correlated. Furthermore, (nearly) normally distributed endogenous regressors cause model non-identification or finite-sample poor performance. Our proposed two-stage copula endogeneity correction (2sCOPE) method simultaneously overcomes the two key limitations and yields consistent causal-effect estimates with correlated endogenous and exogenous regressors as well as normally distributed endogenous regressors. 2sCOPE employs generated regressors derived from existing regressors to control for endogeneity, and is straightforward to use and broadly applicable. Moreover, we prove that exploiting correlated exogenous regressors can address the problem of insufficient regressor non-normality, relax identification requirements and improve estimation precision (by as much as ∼50% in empirical evaluation). Overall, 2sCOPE can greatly increase the ease of and broaden the applicability of instrument-free methods for dealing with regressor endogeneity. We demonstrate the performance of 2sCOPE via simulation studies and an empirical application.]]></description><author>Fan Yang, Yi Qian, Hui Xie</author><pubDate>Sat, 01 Jan 2022 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29708</guid></item><item><title>Two-Way Fixed Effects and Differences-in-Differences with Heterogeneous Treatment Effects: A Survey</title><link>https://www.nber.org/papers/w29691</link><description><![CDATA[<i>Clément de Chaisemartin, Xavier D&#039;Haultfoeuille.</i> <p> <b>January 2022.</b> <p> Linear regressions with period and group fixed effects are widely used to estimate policies' effects: 26 of the 100 most cited papers published by the American Economic Review from 2015 to 2019 estimate such regressions. It has recently been show that those regressions may produce misleading estimates, if the policy's effect is heterogeneous between groups or over time, as is often the case. This survey reviews a fast-growing literature that documents this issue, and that proposes alternative estimators robust to heterogeneous effects.]]></description><author>Clément de Chaisemartin, Xavier D&amp;#039;Haultfoeuille</author><pubDate>Sat, 01 Jan 2022 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29691</guid></item><item><title>Urban Mobility and the Experienced Isolation of Students and Adults</title><link>https://www.nber.org/papers/w29645</link><description><![CDATA[<i>Cody Cook, Lindsey Currier, Edward L. Glaeser.</i> <p> <b>January 2022.</b> <p> Do urban children live more segregated lives than urban adults? Using cellphone location data and following the ‘experienced isolation’ methodology of Athey et al. (2021), we compare the isolation of students over the age of 16—who we identify based on their time spent at a high school—and adults. We find that students in cities experience significantly less integration in their day-to-day lives than adults. The average student experiences 27% more isolation outside of the home than the average adult. Even when comparing students and adults living in the same neighborhood, exposure to devices associated with a different race is 20% lower for students. Looking at more broad measures of urban mobility, we find that students spend more time at home, more time closer to home when they do leave the house, and less time at school than adults spend at work. Finally, we find correlational evidence that neighborhoods with more geographic mobility today also had more intergenerational income mobility in the past. We hope future work will more rigorously test the hypothesis that different geographic mobility patterns for children and adults can explain why urban density appears to boost adult wages but reduce intergenerational income mobility.]]></description><author>Cody Cook, Lindsey Currier, Edward L. Glaeser</author><pubDate>Sat, 01 Jan 2022 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29645</guid></item><item><title>Simple Allocation Rules and Optimal Portfolio Choice Over the Lifecycle</title><link>https://www.nber.org/papers/w29559</link><description><![CDATA[<i>Victor Duarte, Julia Fonseca, Aaron S. Goodman, Jonathan A. Parker.</i> <p> <b>December 2021.</b> <p> We develop a machine-learning solution algorithm to solve for optimal portfolio choice in a detailed and quantitatively-accurate lifecycle model that includes many features of reality modelled only separately in previous work.  We use the quantitative model to evaluate the consumption-equivalent welfare losses from using simple rules for portfolio allocation across stocks, bonds, and liquid accounts instead of the optimal portfolio choices. We find that the consumption-equivalent losses from using an age-dependent rule as embedded in current target-date/lifecycle funds (TDFs) are substantial, around 2 to 3 percent of consumption, despite the fact that TDF rules mimic average optimal behavior by age closely until shortly before retirement. Our model recommends higher average equity shares in the second half of life than the portfolio of the typical TDF, so that the typical TDF portfolio does not improve on investing an age-independent 2/3 share in equity. Finally, optimal equity shares have substantial heterogeneity, particularly by wealth level, state of the business cycle, and dividend-price ratio, implying substantial gains to further customization of advice or TDFs in these dimensions.]]></description><author>Victor Duarte, Julia Fonseca, Aaron S. Goodman, Jonathan A. Parker</author><pubDate>Wed, 01 Dec 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29559</guid></item><item><title>An Economic Model of Consensus on Distributed Ledgers</title><link>https://www.nber.org/papers/w29515</link><description><![CDATA[<i>Hanna Halaburda, Zhiguo He, Jiasun Li.</i> <p> <b>November 2021.</b> <p> In recent years, the designs of many new blockchain applications have been inspired by the Byzantine fault tolerance (BFT) problem. While traditional BFT protocols assume that most system nodes are honest (in that they follow the protocol), we recognize that blockchains are deployed in environments where nodes are subject to strategic incentives. This paper develops an economic framework for analyzing such cases. Specifically, we assume that 1) non-Byzantine nodes are rational, so we explicitly study their incentives when participating in a BFT consensus process; 2) non-Byzantine nodes are ambiguity averse, and specifically, Knightian uncertain about Byzantine actions; and 3) decisions/inferences are all based on local information. We thus obtain a consensus game with preplay communications. We characterize all equilibria, some of which feature rational leaders withholding messages from some nodes in order to achieve consensus. These findings enrich those from traditional BFT algorithms, where an honest leader always sends messages to all nodes. We also study how the progress of communication technology (i.e., potential message losses) affects the equilibrium consensus outcome.]]></description><author>Hanna Halaburda, Zhiguo He, Jiasun Li</author><pubDate>Mon, 01 Nov 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29515</guid></item><item><title>Understanding Algorithmic Discrimination in Health Economics Through the Lens of Measurement Errors</title><link>https://www.nber.org/papers/w29413</link><description><![CDATA[<i>Anirban Basu, Noah Hammarlund, Sara Khor, Aasthaa Bansal.</i> <p> <b>November 2021.</b> <p> There is growing concern that the increasing use of machine learning and artificial intelligence-based systems may exacerbate health disparities through discrimination. We provide a hierarchical definition of discrimination consisting of algorithmic discrimination arising from predictive scores used for allocating resources and human discrimination arising from allocating resources by human decision-makers conditional on these predictive scores. We then offer an overarching statistical framework of algorithmic discrimination through the lens of measurement errors, which is familiar to the health economics audience. Specifically, we show that algorithmic discrimination exists when measurement errors exist in either the outcome or the predictors, and there is endogenous selection for participation in the observed data. The absence of any of these phenomena would eliminate algorithmic discrimination. We show that although equalized odds constraints can be employed as bias-mitigating strategies, such constraints may increase algorithmic discrimination when there is measurement error in the dependent variable.]]></description><author>Anirban Basu, Noah Hammarlund, Sara Khor, Aasthaa Bansal</author><pubDate>Mon, 01 Nov 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29413</guid></item><item><title>Economic Data Engineering</title><link>https://www.nber.org/papers/w29378</link><description><![CDATA[<i>Andrew Caplin.</i> <p> <b>October 2021.</b> <p> Economic data engineering deliberately designs novel forms of data to solve fundamental identification problems associated with economic models of choice. I outline three diverse applications: to the economics of information; to life-cycle employment, earnings, and spending; and to public policy analysis. In all three cases one and the same fundamental identification problem is driving data innovation: that of separately identifying appropriately rich preferences and beliefs. In addition to presenting these conceptually linked examples, I provide a general overview of the engineering process, outline important next steps, and highlight larger opportunities.]]></description><author>Andrew Caplin</author><pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29378</guid></item><item><title>Probabilistic Prediction for Binary Treatment Choice: with Focus on Personalized Medicine</title><link>https://www.nber.org/papers/w29358</link><description><![CDATA[<i>Charles F. Manski.</i> <p> <b>October 2021.</b> <p> This paper extends my research applying statistical decision theory to treatment choice with sample data, using maximum regret to evaluate the performance of treatment rules. The specific new contribution is to study as-if optimization using estimates of illness probabilities in clinical choice between surveillance and aggressive treatment. Beyond its specifics, the paper sends a broad message. Statisticians and computer scientists have addressed conditional prediction for decision making in indirect ways, the former applying classical statistical theory and the latter measuring prediction accuracy in test samples. Neither approach is satisfactory. Statistical decision theory provides a coherent, generally applicable methodology.]]></description><author>Charles F. Manski</author><pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29358</guid></item><item><title>Refining Set-Identification in VARs through Independence</title><link>https://www.nber.org/papers/w29316</link><description><![CDATA[<i>Thorsten Drautzburg, Jonathan H. Wright.</i> <p> <b>October 2021.</b> <p> Identification in VARs has traditionally mainly relied on second moments. Some researchers have considered using higher moments as well, but there are concerns about the strength of the identification obtained in this way. In this paper, we propose refining existing identification schemes by augmenting sign restrictions with a requirement that rules out shocks whose higher moments significantly depart from independence. This approach does not assume that higher moments help with identification; it is robust to weak identification. In simulations we show that it controls coverage well, in contrast to approaches that assume that the higher moments deliver point-identification. However, it requires large sample sizes and/or considerable non-normality to reduce the width of confidence intervals by much. We consider some empirical applications.  We find that it can reject many possible rotations. The resulting confidence sets for impulse responses may be non-convex, corresponding to disjoint parts of the space of rotation  matrices. We show that in this case, augmenting sign and magnitude restrictions with an independence requirement can yield bigger gains.]]></description><author>Thorsten Drautzburg, Jonathan H. Wright</author><pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29316</guid></item></channel></rss>