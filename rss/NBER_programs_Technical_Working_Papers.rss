<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>RSS by programs: Technical Working Papers</title><link>a link</link><description>RSS by programs: Technical Working Papers</description><language>en-US</language><lastBuildDate>Mon, 10 Jan 2022 09:42:35 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Simple Allocation Rules and Optimal Portfolio Choice Over the Lifecycle</title><link>https://www.nber.org/papers/w29559</link><description><![CDATA[<i>Victor Duarte, Julia Fonseca, Aaron S. Goodman, Jonathan A. Parker.</i> <p> <b>December 2021.</b> <p> We develop a machine-learning solution algorithm to solve for optimal portfolio choice in a detailed and quantitatively-accurate lifecycle model that includes many features of reality modelled only separately in previous work.  We use the quantitative model to evaluate the consumption-equivalent welfare losses from using simple rules for portfolio allocation across stocks, bonds, and liquid accounts instead of the optimal portfolio choices. We find that the consumption-equivalent losses from using an age-dependent rule as embedded in current target-date/lifecycle funds (TDFs) are substantial, around 2 to 3 percent of consumption, despite the fact that TDF rules mimic average optimal behavior by age closely until shortly before retirement. Our model recommends higher average equity shares in the second half of life than the portfolio of the typical TDF, so that the typical TDF portfolio does not improve on investing an age-independent 2/3 share in equity. Finally, optimal equity shares have substantial heterogeneity, particularly by wealth level, state of the business cycle, and dividend-price ratio, implying substantial gains to further customization of advice or TDFs in these dimensions.]]></description><author>Victor Duarte, Julia Fonseca, Aaron S. Goodman, Jonathan A. Parker</author><pubDate>Wed, 01 Dec 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29559</guid></item><item><title>An Economic Model of Consensus on Distributed Ledgers</title><link>https://www.nber.org/papers/w29515</link><description><![CDATA[<i>Hanna Halaburda, Zhiguo He, Jiasun Li.</i> <p> <b>November 2021.</b> <p> In recent years, the designs of many new blockchain applications have been inspired by the Byzantine fault tolerance (BFT) problem. While traditional BFT protocols assume that most system nodes are honest (in that they follow the protocol), we recognize that blockchains are deployed in environments where nodes are subject to strategic incentives. This paper develops an economic framework for analyzing such cases. Specifically, we assume that 1) non-Byzantine nodes are rational, so we explicitly study their incentives when participating in a BFT consensus process; 2) non-Byzantine nodes are ambiguity averse, and specifically, Knightian uncertain about Byzantine actions; and 3) decisions/inferences are all based on local information. We thus obtain a consensus game with preplay communications. We characterize all equilibria, some of which feature rational leaders withholding messages from some nodes in order to achieve consensus. These findings enrich those from traditional BFT algorithms, where an honest leader always sends messages to all nodes. We also study how the progress of communication technology (i.e., potential message losses) affects the equilibrium consensus outcome.]]></description><author>Hanna Halaburda, Zhiguo He, Jiasun Li</author><pubDate>Mon, 01 Nov 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29515</guid></item><item><title>Understanding Algorithmic Discrimination in Health Economics Through the Lens of Measurement Errors</title><link>https://www.nber.org/papers/w29413</link><description><![CDATA[<i>Anirban Basu, Noah Hammarlund, Sara Khor, Aasthaa Bansal.</i> <p> <b>November 2021.</b> <p> There is growing concern that the increasing use of machine learning and artificial intelligence-based systems may exacerbate health disparities through discrimination. We provide a hierarchical definition of discrimination consisting of algorithmic discrimination arising from predictive scores used for allocating resources and human discrimination arising from allocating resources by human decision-makers conditional on these predictive scores. We then offer an overarching statistical framework of algorithmic discrimination through the lens of measurement errors, which is familiar to the health economics audience. Specifically, we show that algorithmic discrimination exists when measurement errors exist in either the outcome or the predictors, and there is endogenous selection for participation in the observed data. The absence of any of these phenomena would eliminate algorithmic discrimination. We show that although equalized odds constraints can be employed as bias-mitigating strategies, such constraints may increase algorithmic discrimination when there is measurement error in the dependent variable.]]></description><author>Anirban Basu, Noah Hammarlund, Sara Khor, Aasthaa Bansal</author><pubDate>Mon, 01 Nov 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29413</guid></item><item><title>Economic Data Engineering</title><link>https://www.nber.org/papers/w29378</link><description><![CDATA[<i>Andrew Caplin.</i> <p> <b>October 2021.</b> <p> Economic data engineering deliberately designs novel forms of data to solve fundamental identification problems associated with economic models of choice. I outline three diverse applications: to the economics of information; to life-cycle employment, earnings, and spending; and to public policy analysis. In all three cases one and the same fundamental identification problem is driving data innovation: that of separately identifying appropriately rich preferences and beliefs. In addition to presenting these conceptually linked examples, I provide a general overview of the engineering process, outline important next steps, and highlight larger opportunities.]]></description><author>Andrew Caplin</author><pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29378</guid></item><item><title>Probabilistic Prediction for Binary Treatment Choice: with Focus on Personalized Medicine</title><link>https://www.nber.org/papers/w29358</link><description><![CDATA[<i>Charles F. Manski.</i> <p> <b>October 2021.</b> <p> This paper extends my research applying statistical decision theory to treatment choice with sample data, using maximum regret to evaluate the performance of treatment rules. The specific new contribution is to study as-if optimization using estimates of illness probabilities in clinical choice between surveillance and aggressive treatment. Beyond its specifics, the paper sends a broad message. Statisticians and computer scientists have addressed conditional prediction for decision making in indirect ways, the former applying classical statistical theory and the latter measuring prediction accuracy in test samples. Neither approach is satisfactory. Statistical decision theory provides a coherent, generally applicable methodology.]]></description><author>Charles F. Manski</author><pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29358</guid></item><item><title>Refining Set-Identification in VARs through Independence</title><link>https://www.nber.org/papers/w29316</link><description><![CDATA[<i>Thorsten Drautzburg, Jonathan H. Wright.</i> <p> <b>October 2021.</b> <p> Identification in VARs has traditionally mainly relied on second moments. Some researchers have considered using higher moments as well, but there are concerns about the strength of the identification obtained in this way. In this paper, we propose refining existing identification schemes by augmenting sign restrictions with a requirement that rules out shocks whose higher moments significantly depart from independence. This approach does not assume that higher moments help with identification; it is robust to weak identification. In simulations we show that it controls coverage well, in contrast to approaches that assume that the higher moments deliver point-identification. However, it requires large sample sizes and/or considerable non-normality to reduce the width of confidence intervals by much. We consider some empirical applications.  We find that it can reject many possible rotations. The resulting confidence sets for impulse responses may be non-convex, corresponding to disjoint parts of the space of rotation  matrices. We show that in this case, augmenting sign and magnitude restrictions with an independence requirement can yield bigger gains.]]></description><author>Thorsten Drautzburg, Jonathan H. Wright</author><pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">https://www.nber.org/papers/w29316</guid></item></channel></rss>