<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>RSS for CEPR &lt;built-in method strip of str object at 0x7f7719f7dad0&gt;</title><link>a link</link><description>RSS for CEPR &lt;built-in method strip of str object at 0x7f7719f7dad0&gt;</description><language>en-US</language><lastBuildDate>Fri, 17 Dec 2021 16:12:38 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Non-Standard Errors</title><link>http://cepr.org/active/publications/discussion_papers/dp.php?dpno=16751</link><description><![CDATA[<i>Christian C P Wolff Utz Weitzel Michael Razen Sebastian Neusüess Michael Kirchler Magnus Johannesson Juergen Huber Felix Holzmeister Anna Dreber Albert Menkveld.</i> <p> <b>November 11, 2021.</b> <p> In statistics, samples are drawn from a population in a data generating process (DGP). Standard errors measure the uncertainty in sample estimates of population parameters. In science, evidence is generated to test hypotheses in an evidence generating process (EGP). We claim that EGP variation across researchers adds uncertainty: non-standard errors. To study them, we let 164 teams test six hypotheses on the same sample. We find that non-standard errors are sizeable, on par with standard errors. Their size (i) co-varies only weakly with team
merits, reproducibility, or peer rating, (ii) declines significantly after peer-feedback, and (iii) is underestimated by participants.]]></description><author>Christian C P Wolff Utz Weitzel Michael Razen Sebastian Neusüess Michael Kirchler Magnus Johannesson Juergen Huber Felix Holzmeister Anna Dreber Albert Menkveld</author><pubDate>Tue, 23 Nov 2021 00:00:00 GMT</pubDate><guid isPermaLink="true">http://cepr.org/active/publications/discussion_papers/dp.php?dpno=16751</guid></item></channel></rss>